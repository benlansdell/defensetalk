<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Unraveling principles of motor control: from nerve nets to neural prosthetics</title>
    <meta name="viewport"
	  content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <!--<link rel="stylesheet" href="reveal.js/css/reveal.min.css">-->
    <link rel="stylesheet" href="reveal.js/css/reveal.min.css">
    <!--<link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">-->
    <link rel="stylesheet" href="cust_black.css" id="theme">
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
    <script>
      document.write('<link rel="stylesheet" href="reveal.js/css/print/'+(window.location.search.match(/print-pdf/gi) ? 'pdf' : 'paper')+'.css" type="text/css" media="print">');
    </script>
    <script src="js/three.js"></script>
    <!--[if lt IE 9]>
    <script src="reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->

<!-- Printing and PDF exports -->
<script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>
  </head>

  <body>
    <!-- Slides! -->

    <div class="reveal">
      <div class="slides">
        <section data-background="assets/dst2_background.png">
          <h1>Unraveling principles of motor control: from nerve nets to neural prosthetics</h1>
	  <br><br>
	  <p style="text-align: center; font-size: larger; text-shadow: 0px 0px 6px #000000;">Ben Lansdell<br> University of Washington - Applied Mathematics</p>
	  <aside class="notes">
	    <span style="color: red">
	    </span> •
	    • <span style="color: green"></span>
	  </aside>
        </section>

<!-- ----------------------------------------------------------------------- -->

	<section data-transition="fade-in">
		<h2>Outline</h2>
		<ol>
			<li class="fragment highlight-green"> Motor encoding in concurrent use brain-computer interfaces
			<li> Whole-animal imaging analysis in cnidarian Hydra
		</ol>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Motor cortical encoding and brain-computer interface (BCI) design</h2>
	    <img src="assets/soccer.png" width="50%">

<ul>
	<li> Motor cortical neuron representations are complex
	<li> Understanding motor encoding can inform design of BCIs for restoring function to paralyzed limbs, or for control of prosthetic limbs
	<li> Optimal BCI performance likely a mix of decoder choice and user adaptation processes
</ul>
	    <aside class="notes">
	    <span style="color: red"></span> •

Encoding of kinetic and kinematic parameters of motor output in primary motor cortex are known to be complex. The source of this complexity is an interesting scientific question, but also one of relevance to the design of brain computer interfaces for the purpose of neural prosthetic control. Optimal BCI performance is likely to be acheived through a mix of how we decode neural activity and an understanding of the adaptation processes that take place within the user.

	    • <span style="color: green"></span>
	  </aside>	</section>

	<section>
		<h2>Intracortical arrays provide state-of-the-art BCI control</h2>
	  <div id="left">
	    <img src="assets/utah.png" width="45%">
	    <img src="assets/motorcortex.png" width="45%">
	    <p class="rcred"></p>
	  </div>
	  <div id="right">
	    <img src="assets/DrinkingMoment.jpg" width="75%">
	  </div>
	    <ul><li>Monkeys can be trained to volitionally control individual neurons through feedback and conditioning [Fetz 1969]
	    	<li>Neurons chosen independently of natural movement association [Moritz and Fetz 2011]
	    	<li>Conversely, other studies report brain-control mappings which utilize activity observed during the natural motor repertoire are most effective [Sadtler et al 2014]
	    </ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
So I'll be talking today about brain computer interfaces that do rely on adaptation of activity of motor cortical activity 

Current state-of-the-art BCI control is achieved via intra-cortical brain signals. With these signals, studies have shown that monkeys can be trained to volitionally control individual neurons through a process of biofeedback and operant conditioning. Thus such BCIs rely on adapation processes.

In these tasks, there is evidence that neural activity can be chosen independently of natural movement association, reflecting the remarkable adaptability of motor cortical activity. However despite this adaptability, other BCI studies observe that brain-control mappings which utilize activity observed during the natural motor repertoire are most effective. These contrasting findings suggest that, provided basic neuronal constraints imposed by existing circuitry are understood, next-generation BCI design may be able to better leverage the brain’s inherent adaptability.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Dual-control BCIs</h2>
	    <img src="assets/dualcontrolBCI.png" width="50%">
	    <ul>
	  		<li> Allow stroke patients to regain functionality through co-opting
	  			healthy motor cortex to control neural prosthetic in conjunction with residual movement 
   		</ul> 
	  <aside class="notes">
	    <span style="color: red"></span> •
Most BCI studies to date focus on control of external devices alone. These do not require concurrent motor output of the subject, which frees the neurons or single units controlling the BCI (henceforth control units) to modify their activity as needed to perform the task. In concurrent use cases, however, in which both motor output and brain control are required to perform a task, much less is known about how control units are able to modify their activity to peform the task. 

Concurrent control of natural movement and BCI output may be useful in cases of brain injury. For example, when the motor cortex of one hemisphere is damaged by stroke or trauma, a BCI-controlled neural prosthetic may be driven by the intact hemisphere. In this case cortical circuits in the intact hemisphere may be tasked with both producing movement of the unaffected limb, while also restoring function to the affected, ipsilateral limb using a BCI. 
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Dual-control BCIs</h2>
	    <img src="assets/easyharddiag2.png" width="50%">
	    <p class="rcred">Milovanovic et al 2015</p>
	    <ul>
   		<li> Do neurons strongly associated with contralateral motion make poor control units due to the potential interference imposed by concurrent hand movement?
   		</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
While subpopulations of neurons encoding ipsilateral motion are a candidate control source for the BCI following stroke, a robust interface may require recording from the larger population of neurons involved in natural, contralateral control. Do neurons strongly associated with contralateral motion make poor control units due to the potential interference imposed by concurrent hand movement? Assuming that the brain area recorded from has some causal role in motor control, it seems that at least some neurons could present challenges for BCI control for this reason.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Dual-control BCIs</h2>
		<img src="assets/dc_perf.png" width="55%">
		<p class="rcred">Milovanovic et al 2015</p>
	    <ul>
	    	<li> Previous studies show performance independent of unit tuning [Milovanovic et al 2015]
	    </ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
In a previous study we observed that a monkey was able to use all tested pairs of single cortical units to control a BCI task. Overall performance in this dual-control task was similar, regardless of the control units’ directional tuning. Explain figure.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Dual-control BCIs</h2>
	  <div style="text-align: left">
	    <p>Activity of control and non-control units</p>
	  	<ul>
	  		<li> BCIs induce widespread changes in activity and tuning in variety of tasks and task perturbations [e.g. Carmena et al 2003]
  			<li> Some studies show control unit specific changes in tuning [e.g. Law et al 2014]
	  	</ul> 
	  	<p class="fragment">How do these effects manifest in a BCI paradigm where control units may be constrained by their role in ongoing movement?</p>

	  <aside class="notes">
	    <span style="color: red"></span> •
How is this control achieved? Does this adaptability occur only for the control units? Or it requires population changes in activity?

Previous studies of the behavior of control and non-control unit activity reveal widespread changes in activity and tuning preferences in a variety of tasks and task perturbations. In addition to these changes other studies showed specific changes for the control units compared with those not involved in the control. How do these unit specific effects manifest in a BCI paradigm where control units may be additionally constrained by their role in ongoing movement?
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Experiment</h2>
<img src="assets/Figure 1_monkey_ver3.png">
	<ul>
		<li> Utah multi-electrode array implanted in hand/wrist area of primary motor cortex of Macaque monkey
	  	<li class="fragment">
	  	Chet Moritz, Ivana Milovanovic, Cooper Mellama, Adrienne Fairhall (UW)
	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
In this study we design a dual-control BCI that requires both BCI control and ongoing motor output are driven by the same cortical region, requiring control units to dissociate their activity from the circuits controlling the movement. The task requires that a monkey acquire a target in two dimensions, one dimension controlled by a BCI while the other is controlled using natural motor control of the wrist contralateral to the recorded hemisphere (Figure 1).

This task allows us to investigate how primary motor cortical neural activity can be decoupled from natural motion. Other concurrent-use BCIs demonstrate similar robustness, albeit often with some adaptation period required. Here we examine whether dissociation from wrist motion occurs specifically for control units, or whether this dissociation is associated with changes in tuning, functional connectivity at the population level. 

We also compare the motor cortical activity observed during the dual-control task to a standard brain-control task which requires the monkey acquire targets only through BCI control. To determine baseline tuning, variability and connectivity of the recorded population, the monkey first performed a manual control (MC) target-pursuit task that only involved acquiring targets through wrist motion. When compared to this manual control task, we observe widespread changes in tuning of both control and non-control units for the brain-control and dual-control tasks.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Random target pursuit task</h2>
			<ul>
				<li> Target appears randomly outside radius from cursor position
				<li> Acquire target within fixed time, hold for 1s
			</ul>
	    <img src="assets/targettask.png" width="70%">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  		<p>Simple linear tuning model:
	  		$$ n^i_t = \alpha_x x_t + \alpha_y y_t + c + \epsilon_t$$</p>
	  		<ul>
	  		<li>Provides measure of preferred tuning angle $\theta$ and tuning strength<br>
	  		<li>Units chosen with preferred tuning 90 rotated from dual control direction
	  		</ul>
	    <img src="assets/fig2a.png">
	    <ul>
	    	<li>Widespread changes in unit tuning between conditions, for control and non-control units alike
	    </ul> 
	  <aside class="notes">
	    <span style="color: red"></span> •
During both the brain control and dual control tasks we measure how units change their tuning to wrist motion, and compared this tuning to that observed during the manual control task. Our goal was to understand the overall differences in neural activity that underlie performance of the brain-control task compared to the dual-control task where wrist motion is also required. By fitting a linear tuning model to each unit throughout a session, we first analyzed changes in tuning to wrist motion between different tasks. The linear model provides a simple measure of a unit’s preferred wrist velocity, given by a tuning angle and tuning strength (see Methods). 

When comparing to the manual control task, we observed significant changes in average tuning angle (p < 0.001; two sided t-test, Figure 2A,B) and average tuning strength (p < 0.001, Figure 2C) for units in both the brain control and dual control task. To minimize the possibility that 90o of visual feedback rotation could by itself result in observed changes in tuning angle, we also conducted a control task, where visual feedback was not rotated, and observed similar results (Figure S2).
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	    <img src="assets/fig2bII.png" width="70%">
	    <p>Compared to activity during manual control</p>
	    <ul>
	    	<li> Brain-control:<ul>
	    		<li>44% control units significantly change tuning angle
	    		<li>28% non-control units significantly change tuning angle
	    	</ul>
	    	<li> Dual-control:<ul>
	    		<li>21% control units significantly change tuning angle 
	    		<li>19% non-control units significantly change tuning angle
	    	</ul>
	    </ul> 
	  <aside class="notes">
	    <span style="color: red"></span> •
We found that during the brain control task, 56% of control units and 72% of non-control units did not significantly change their tuning angle compared to the manual control task. However, in dual control, unit tuning angles were more constrained by wrist motion, with 79% of control units and 81% of the non-control units having a similar tuning angle as in manual control (Figure 2 A,B). Indeed, during the brain control task there was a significantly greater change in the tuning angle of the control units compared to non-control units (p=0.008, two sided t-test). In contrast, specific modification of control unit tuning was not observed in dual control, where preferred direction changed similarly for both types of units (p=0.699, two sided t-test). 
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	    <img src="assets/fig2c.png" width="70%">
	    <ul>
	    <li>No control unit specific changes in tuning strength
	    </ul> 
	  <aside class="notes">
	    <span style="color: red"></span> •
To assess possible lasting effects of previous brain control trials, the monkey performed another manual control task after the dual control task. During this manual control session, hereafter the post-manual control session, the majority of units had the same tuning angle as in the first manual control task (correlation coefficient 0.99; Figure 2A). 
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	    <img src="assets/connectivity_all.png" width="70%">
	  <aside class="notes">
	    <span style="color: red"></span> •
We next considered how the relationships between pairs of units depended on the task. To evaluate this, we used a measure of functional connectivity, transfer entropy. The transfer entropy of two random processes, from x_t to y_t, measures the information about y_t obtained by observing the history of x_t, conditioned on the history of y_t (see methods). We note that functional connectivity is a statistical relationship and should not be taken to imply synaptic connection.,
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	    <img src="assets/connectivity_to_one.png" width="70%">

	  <aside class="notes">
	    <span style="color: red"></span> •
We investigated the relation between control units and co-tuned unit activity during the dual control task. As described above, in dual control most units maintain their association with wrist motion observed in manual control. However, the control units are required to modulate their activity independently of this association. Thus we might expect that they will subsequently exhibit less of a relation with units with which they were previously co-tuned with, and that this will manifest as a change in the functional connectivity between control and co-tuned units. According to this hypothesis the dual-control task requires control unit activity to become specifically less dependent on co-tuned neural activity. We therefore used a directed measure of functional connectivity, transfer entropy, to measure this independence.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<p>
	  	For a fixed network of units, compute transfer entropy between units in each condition:
	  	$$
	  	H_{X\to Y} = I(Y_t|Y_{t-1}, \dots, Y_{t-T}) - I(Y_t|Y_{t-1}, \dots, Y_{t-T},X_{t-1}, \dots, X_{t-T})
	  	$$
	  	for Shannon entropy $I$. <br><br>

	  	Study differences in connectivity between brain-, dual- conditions and manual condition.<p>
	    
	    <p style="text-align: center"><img src="assets/te_changes.png" width="70%" style="text-align: center"></p>

	  <aside class="notes">
	    <span style="color: red"></span> •
We focused on changes in connectivity that occurred between manual control and brain or dual control tasks for different subpopulations of unit pairs (see Methods). Overall, we observed a widespread change in the transfer entropy between manual control and the brain and dual control conditions. This change was smallest between the pre- and post- manual recordings, with the brain and dual control conditions showing larger changes in functional connectivity (Figure 3A). The changes for both brain control (R2 = 0.212) and dual control (R2 = 0.221) were similar (Figure 3A). 
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	    <img src="assets/TE_part1.png" width="70%">

	  <aside class="notes">
	    <span style="color: red"></span> •
We then compared the change in connectivity for control units versus non-control units. We observed that between brain and manual control, control units decreased their connectivity significantly more than non-control units. We observed the same trend when comparing changes in connectivity between dual and manual control (Figure 3B). Thus in both brain and dual control tasks, the control units become more independent from units than non-control units -- both tasks result in some control unit-specific dissociation.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	    <img src="assets/TE_part2.png" width="70%">

	  <aside class="notes">
	    <span style="color: red"></span> •
We wondered if this control unit dissociation was specific to units that were co-tuned with the control unit. Indeed, comparing changes in connectivity to the control unit between brain and manual control showed a significant difference in the connectivity change for co-tuned units versus randomly selected units (p-value << 0.001 (1.5288x10-6) [rank sum] ; Figure 3C). However, comparing such changes in connectivity to the control unit between dual control and manual control does not reveal a significant difference (p-value = 0.5287 [rank sum]). Thus control units become more independent from co-tuned units than to other units in the brain control task, but not in the dual control task. 
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	    <img src="assets/TE_part3.png" width="70%">
	  <aside class="notes">
	    <span style="color: red"></span> •
This result is counterintuitive, given that the structure of the dual-control task was designed to effect the opposite result -- control unit specific independence. The possibility remains, however, that the brain control task leads to a higher degree of change in co-tuned units in general, and that the effect has nothing to do with whether or not a control unit is involved. Thus we also examined changes in connectivity to non-control units, from either co-tuned units or from non-co-tuned units (Figure 3D). In the brain control task, connectivity decreases significantly more for co-tuned units than for non-co-tuned units. This is similar to the result obtained in Figure 3C for control units. In dual control, however, connectivity changes significantly less for co-tuned units than for non-co-tuned units. In fact, between manual and dual control the overall change in connectivity for co-tuned units to non-control units is not significantly different from zero (p-value = 0.145). Thus, in dual control functional connectivity between co-tuned units is maintained, unless one of the units is a control unit, while in brain control there is an overall decrease in connectivity for co-tuned units, regardless independently of whether a control unit is involved.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Changes in functional connectivity of population</h2>
	  	<ul>
	  		<li> Brain-control: overall decrease in functional connectivity to control units
	  		<li> Dual-control: functional connectivity between co-tuned units does not change, except when control unit involved
	  	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
This result is consistent with the tuning angle findings of the previous section, in which greater changes in tuning angle were observed in brain control compared to the changes in dual control (Figure 2B). Together these findings suggest that the manual component of the dual-control task holds both the tuning angles and connectivities between co-tuned units in similar states to that observed in manual control, except for connectivities involving the control units.
• <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Intrinsic variability predicts performance</h2>
		<p>Inspired by Sadtler et al 2014</p>
	    <img src="assets/intrinsic_manifold_white.png" width="70%">

	  <aside class="notes">
	    <span style="color: red"></span> •
Despite the insight that the connectivity and tuning analysis provide into the neural activity that underlies the dual control task, neither analysis was predictive of task proficiency. This is consistent with previous studies showing remarkable flexibility of motor cortical activity. Previous studies also demonstrate that neural variability measured during natural behavior -- intrinsic variability -- is a constraint on learning and proficiency of brain control tasks [11 -- Sadtler et al 2014]. Thus we investigated the role of intrinsic variability of the control neurons in relation to the variability of the entire neural population.
• <span style="color: green"></span>
	  </aside>
	</section>

	<section>
		<h2>Intrinsic variability predicts performance</h2>
<ul>
<li> GPFA used to identify low-dimensional subspace
<li> Identify when spaces are significantly non-orthogonal
</ul>
	    <img src="assets/gpfa_perf.png" width="30%">
	  <aside class="notes">
	    <span style="color: red"></span> •
We began by identifying a low-dimensional space that characterizes a high proportion of the variance observed during natural movements from the manual control recordings. We used Gaussian process factor analysis (GPFA) to identify this space, which we term the intrinsic space (similar to the intrinsic manifold proposed by Sadtler et al 2014 [11 -- Sadtler et al 2014]). In line with this terminology, we term the variance of each recorded unit during the manual control task its intrinsic variability. When a brain control task is performed, the activity from just two neurons determines the cursor position. The activity of these two neurons thus comprises a brain-control space (Figure 4A). Based on [11 -- Sadtler et al 2014], we hypothesized that more proficient brain-control and dual-control would occur when the brain-control space and the intrinsic space share some amount of alignment. We observed that in the brain control task, but not in the dual control task, performance is significantly higher when the brain-control and intrinsic spaces are indeed non-orthogonal (Figure 4B).	

Need to mention that when we try GLM type encoding models, as well as linear models,
we observe no relation between tuning and performance.<br>
    • <span style="color: green"></span>
	  </aside>
	</section>

<!-- ----------------------------------------------------------------------- -->

	<section>
	  <h2>Intrinsic variability predicts performance</h2>
	    <img src="assets/Figure5_ver2c.png" width="60%">
	    <ul>
	    	<li> High performance requires at least one unit with high intrinsic variance
	    </ul> 
	  <aside class="notes">
	    <span style="color: red"></span> •
To measure each unit’s contribution to the target acquisition task, we constructed a metric which quantifies the amount of influence each control unit has over the cursor trajectory. Since the cursor trajectory was determined through a moving average model weighing the contribution from each control unit, we selected a Granger causality-type metric (see methods). We validated this metric using synthetic cursor data generated through a weighted contribution of two neurons (Figure S4). Not surprisingly, we observed that a high value of this control metric for at least one of the control neurons is required for dual control task proficiency (Figure 5A).
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Summary</h2>
	  	<ul>
	  		<li> Tuning and connectivity analysis suggest dual-control task generates cortical activity more similar to that observed in manual control task
	  		<li> Intrinsic variance of control units only variable found to predict performance and control unit contributions &#8212; motor unit tuning does not constrain how the task is performed
	  	</ul>
	  	<p>$\Rightarrow$ As observed with other BCIs: provided basic neuronal constraints imposed by existing circuitry are taken into consideration, BCI design is able to leverage motor cortical adaptability.</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section data-transition="fade-in">
		<h2>Outline</h2>
		<ol>
			<li> Motor encoding in concurrent use brain-computer interfaces
			<li class="fragment highlight-green"> Whole-animal imaging analysis in cnidarian Hydra
		</ol>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section data-background-color="#000000">
	<h2>Whole-animal imaging in unconstrained Hydra</h2>
	  <div>
	<video src="assets/hydra_feeding_short.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="60%"></video>
	 <p class="rcred">https://www.youtube.com/watch?v=dl_oVns2oa8</p>
	 <div class="fragment">
	<p>Why?</p>
	 <ol>
	 	<li> Small (0.5mm-1.5cm) &#8212; can fit into field of view of traditional microscope
	 	<li> Translucent; nerve net, easier imaging
	 	<li> Does not age, and can regenerate
	 </ol>
	</div>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    The Hydra has a diffuse nerve net, one of the simplest nervous systems.

	    As part of a collaboration with Yuste lab at Columbia, among others, we
	    are working on obtaining this level of knowledge in the cnidarian Hydra.

	    For this, the Hydra has the advantages that:
	    -it is transparent, making imaging simpler
	    -it is small, allowing it to fit into the FOV of a traditional scope
	    -it consists of a nerve net, meaning that neurons are spread throughout
	    the organism and can be imaged more or less without obstructing one another
	    -it can be constrained to lie in a 2D plane, meaning more or less the organism
	    can be in focus all at once, without the need for z-slicing, which permits a high
	    temporal resolution recording.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Hydra anatomy</h2>
	  <div>
	    <img src="assets/hydra_layers_white.png" width="100%" position="left">
	    <p class="rcred">Adapted from Technau and Steele 2011</p>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    Describe Hydra anatomy

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Aims</h2>
<hr>Understand (and control) neuronal basis of simple behavior such as contracting/elongating and expelling
	    fluid</p>
	     <div>
<hr><p>Sub-aims:</p>
	  	<ol>
	  	<li class="fragment highlight-green">Track Hydra pose</li>
	  	<li>Register and track neurons</li>
	  	<li>Record neural activity</li>
	  </ol>
	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • Aims: understand (and control) simple behavior such as contracting and expelling
	    water.

	    Hydra periodically absorbs water (possibly through osmosis) and expells it all at once
	    once reaching a certain size. Nerve free Hydra do not expell fluid, so we know this is
	    a nervous system function.	

	    Sub aim:
	    In achieving this, a sub-aim is to be able to track the Hydra's pose and neurons throughout
	    unconstrained behavior. This allows analysis of the Hydra's behavior, it's neural activity,
	    and of course the relationship between the two.

	    I will spend the 1st half of this talk describing a method for performing this tracking in
	    videos of Hydra in which neurons have been tagged with GCamp6, allowing for their neural
	    activity to be imaged. 

	    Mention this is a collaboration with Rafa Yuste lab in Columbia
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Experiment</h2>
	  <div id="left">
	    <img src="assets/hydrasetup.jpg" width="50%">
	    <p class="rcred"></p>
	  </div>
	  <div id="right">
	    <img src="assets/hydrasetup3.jpg" width="100%" position="left">
	    <p class="rcred">C. Dupre, Yuste lab</p>
	  </div>
	    <p>Methods</p>
	    <ul>
	    <li> Create Act-GCaMP6s transgenic Hydra
		<li> Mount between coverslips separated by .1mm spacer
		<li> Image calcium transients
		</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    Methods
	    - Hydra details
	    Genetic calcium indicator. Modified plasmid which expresses GFP, to express GCaMP6.
	    Injected in Hydra fertilized eggs. (Act-GCaMP6s transgenic Hydra)
	    - imaging methods
	    mounted specimens between two coverslips separated by .1mm spacer. Hollow cylinder
	    imaged from the side. Transparent means can image activity of every one of its neurons.
	    - experiment details:
	    transients digital -- all or none -- fast risetimes and slow decays. Hydra neurons have
	    action potentials, express sodium channels in their genome, simplest explanation
	    of changes in flouresence is due to calcium influx due to action potential activity.
	    As in mammalian neurons. Did not record Ca signals of intermediate amplitude suggests that
	    capturing individual action potentials.

	    • <span style="color: green"></span>
	  </aside>
	</section>

<!-- ----------------------------------------------------------------------- -->

	<section>
	<h2>Whole-body calcium imaging in Hydra</h2>
	<video src="assets/hydra_gcamp.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="70%"></video>
	 <p class="rcred">C. Dupre, Yuste lab</p>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    How do we go about doing this tracking?

	    Some issues:
	    - neuron tracking (which if reliable would provide information about neural
	    activity and body position) is problematic since not all neurons are visible
	    at all times.
	    - particle tracking thus has a track association problem
	    - how are neurons reassociated once they become inactive then active again?

	    As an aside, note that in the future we may have access to labeled neurons
	    through the use of nanoparticles or genetic labels. This would greatly aid in the
	    neuron tracking and body tracking by provided clear fiducial markers that can be consistently
	    tracked. However, this is not available yet and remains an experimental
	    challenge. Further, not all Hydra strains may have such labeling, and we would like
	    methods that can tracking Hydra body in cases when we have flourescent labels for
	    activity in epithelial cells which show muscle activity. In these datasets, individual
	    cells may be hard to track and make out. Since we would like to do behavior analysis on
	    these dataseta also, we will here tackle the problem of performing body tracking
	    in cases where explicit fiducial markers are not present.

	    For today let's take this as our problem.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section data-background-color="#000000">
	<h2>Deformable object tracking</h2>
	<div id="left">
	<video src="assets/kalmanfilter_slow.mp4" muted controls
	 poster="assets/dst2.png" class="slideautostart"></video>
	</div>
	<div id="right">
		<p>Extended Kalman filter tracking

			$$\begin{align}y_t &= h(x_t) +\nu_t \\
			x_t &= f(x_{t-1}) +\epsilon_t\end{align} $$ 

			Where $x_t$ is the Hydra positions and velocities, $y_t$ is the generated
			image, $f(\cdot)$ is mass-spring dynamics, $h(\cdot)$ observation.<br> <br>
		</p>
		<ul class="fragment">
			<li>High-dimensional &#8212; slow, even if using OpenGL/CUDA<br>
			<li>Unstable &#8212; once tracking lost, difficult to recover
		</ul>

	</div>
	  <aside class="notes">
	    <span style="color: red"></span> •

	    How do we go about doing this tracking?

	    A solution is to then fit a parameterization or mesh to the body explicitly
	    and to use this to perform neuron tracking.

	    We can do this by adapting methods from deformable object tracking.

	    However, as a very high dimensional state space model. these methods are slow, unstable, and difficult to recover from difficult Hydra motion. Once it's lost the Hydra, it can't recover.

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Multi-frame optic flow image registration</h2>
	<img src="assets/mfsf_white.svg">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Multi-frame optic flow image registration</h2>
	<ul id="blanklist">
	<li> Dense optic flow with subspace constraints (MFSF)
	<li> For each pixel find linear combination of basis paths, $L$, that minimize energy<br>
	<img src="assets/energyeq.png">
	<li> $I_f$ = image at frame $f$,
	<li> $I_0$ = reference frame (need not be first frame of video),
	<li> $Q_f^u$, $Q_f^v$ = basis paths at frame $f$, 
	<li> $\alpha$ = smoothness regularizer
	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Tracking with optic flow</h2>
	<video src="assets/warp_neurons_nref100.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart"></video>
	 <ul> 
	 <li> blue = hand tracked neurons
	 <li> green = w/in 6px of 'true', red = >6px of 'true'
	 </ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	   	We can see that this work fairly well for short sequences of video.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Tracking with optic flow: performance</h2>
		<div id="left">
		<p>Comparison to hand annotated neuron tracks<br>
		Per frame:</p>
		<ul>
		<li> at least 52% neurons are tracked within 6px
		<li> on average 82% neurons tracked within 6px
		</ul>
		Per neuron:
		<ul>
		<li> 42% neurons tracked within 6 px throughout all video
		</ul>
		</div>
		<div id="right">
		<img src="assets/tracking_errors.png">
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    This method works fairly well for short sequences of video. Tracking 42%
	    of all neurons is high enough to start to be useful, even if there's plenty
	    of room for improvement.

	    Further, it is fast and stable.

	    However, it is not always appropriate to use the one reference frame for the
	    Hydra, since it changes position quite significantly throughout a recording.
	    (show video)
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Registering in longer sequences</h2>
		<img src="assets/mfsf_stitch_white.svg" width="70%">
		<div class="fragment">
		How to mitigate accumulation of errors from video to video?
		</div>
	  <aside class="notes">
	    <span style="color: red"></span> •
		Changing appearance/pose in a long sequence → multiple reference frames
		Simplest approach: split into multiple stacks and run independently
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Choosing which paths to associate</h2>
	Measure image registration error
		<img src="assets/combined.png">
		<div>
		<div class="column-left">Forward map $$g_{1,2}(\mathbf{x})$$</div>
		<div class="column-center">Reverse map $$g_{2,1}(\mathbf{x})$$</div>
		<div class="column-right">Error $$f_{1,2}(\mathbf{x}) = \left|g_{2,1}(g_{1,2}(\mathbf{x}))\right|$$</div>
		</div>
		<br><img src="assets/colorwheel.png" align="left">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>


	<section>
	<h2>Exploiting periodicity in Hydra behavior</h2>
	<div id="left">
	<ul> 
	<br><br><br>
	Stereotyped Hydra behavior:<br>elongate then contract<br><br>
	Compute and measure optic flow error, $f_{ij}(x)$, between frames with regular spacing
	<br><br>'iframes' &#8212; every 250 frames
	</ul>
	</div>
	<div id="right" style="text-align: center;">
		$\langle f_{ij}(x)\rangle_\Omega$
		<img src="assets/similarity_orig.png">
		Frame index
	</div>

	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Exploiting periodicity in Hydra behavior</h2>
	<div id="left">
	<p> 
	Two clusters: contracted and elongated<br>
	$\Rightarrow$ By registering regions of each iframe with a ref. frame we extend paths into temporally distant but positionally related frames<br>
	$\Rightarrow$ A mechanism to handle arbitrarily long videos without accumulation of tracking error
	</p>
	</div>
	<div id="right" style="text-align: center;">
		$\langle f_{ij}(x)\rangle_\Omega$<br>
		<img src="assets/dendrogram.png" width="70%">
	</div>
		<img src="assets/dend_d1_tile_c1.png" width="80%">
		<img src="assets/dend_d1_tile_c2.png" width="80%">
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Segmenting regions to register with each reference frame</h2>
		<p>For $K$ reference frames and $L$ iframes, let $f_{ij}(x)$ represent the optic flow error in using reference image $i$ to construct image $j$.<br>
		Use total variation image segmentation:
		$$\begin{align*}\min_{u_{kl}}\frac{1}{2}\sum_{k=1}^K\sum_{l=1}^L \int_\Omega |\nabla u_{kl}|\,dx
			+ \frac{\lambda}{2}\sum_{k=1}^K\sum_{l=1}^L \int_{\Omega} u_{kl}(x)f_{kl}(x)\,dx\end{align*}
		$$
		Solve with Chambolle algorithm, implemented on GPU.
		</p>
	<p class="fragment">
		Want number of ref frames to balance global registration vs registration error<br>
		$\Rightarrow$ Add a group LASSO penalty for number of reference frames used:
		$$\begin{align*}\min_{u_{kl}}\frac{1}{2}\sum_{k=1}^K\sum_{l=1}^L \int_\Omega |\nabla u_{kl}|\,dx
			+ \frac{\lambda}{2}\sum_{k=1}^K\sum_{l=1}^L \int_{\Omega} u_{kl}(x)f_{kl}(x)\,dx+ \frac{\lambda_2}{2}\sum_{k=1}^K\left(\sum_{l=1}^L \|u_{kl}\|^2_2 \right)^{1/2}\end{align*}
		$$
	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			1. Select very sparse set of reference frames (ref frames)<br>
			2. Select regular set of inter-frames (iframes)<br>
			<img src="assets/dend_d1_tile_c1_hl.png" width="90%"><br>
			<img src="assets/dend_d1_tile_c2_hl.png" width="90%"><br>
			3. Use optic flow+image segmentation to label regions mapping to reference frames<br>
			<img src="assets/mfsf_extend1.png" width="90%"><br>
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			4. Within each iframe block run MFSF for dense registration<br>
			<img src="assets/mfsf_extend2.png" width="90%"><br>
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Proposed method</h2>
		<p> 
			5. Associate each path from (4) with point in a ref frame using optic flow+segmentation (3)<br>
			<img src="assets/mfsf_extend3.png" width="90%"><br>
		</p>
		<p class="fragment">
			Thus every tracked path is associated with a point in a reference frame
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
		Thus every tracked path is associated with a point in a reference frame
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Extending with $K=2; L = 8$</h2>
	<video src="assets/mfsf_dm_combined2.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart" width="55%"></video>
	  <aside class="notes">
	    <span style="color: red"></span> •
	   	We can see that this work fairly well for short sequences of video.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<!-- <section>
	<h2>Combining reference frames through temporal continuity of neuron tracking</h2>
	<video src="assets/mfsf_dm_combined.mp4" muted controls loop
	 poster="assets/dst2.png" class="slideautostart"></video>
	  <aside class="notes">
	    <span style="color: red"></span> •
	Allows for registration of neurons over videos of indefinite length
	    • <span style="color: green"></span>
	  </aside>
	</section> -->

	<section>
	<h2>Summary</h2>
	<ul>
		<li> Registration between similar frames -- track Hydra pose throughout extended video sequences. Can be applied to other registration/tracking problems
		<li class="fragment"> Create larger hand annotated datasets for performance evaluation
		<li class="fragment"> Exploit temporal continuity between adjacent blocks to relate reference frames to one another
	</ul>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    This approach has utility as a general optic flow technique that can handle occlusions &#8212; an important problem in computer vision.

	    We've seen how this method can be used to track Hydra, which we believe will
	    form an important component of determining the neural basis of its behavior.
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Models of motor cortical function</h2>
	<ul>
		<li> Explicit encoding models of kinetics and kinematics only tell so much
		<li> Internal and latent dynamics reveal additional functionality
	</ul>
	<hr>
	<p><br><br></p>
	"Progress in science depends on new techniques, new discoveries and new ideas, probably in that order" <br>
	&#8212; Sydney Brenner
	  <aside class="notes">
	    <span style="color: red"></span> •

	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	  <h2>Acknowledgments</h2>
	  	<hr>
	  <div id="left">
	  	<ul>
         <li> Adrienne Fairhall</li>
         <li> Chet Moritz</li>
         <li> Ivana Milovanovic</li>
         <li> Cooper Mellema</li>
         <li> Eberhard Fetz</li>
	  	</ul>
   	  </div>
	  <div id="left">
	  	<ul>
         <li> Fairhall lab</li><ul>
         	<li> Anatoly Buchin </li></ul>
         <li> Moritz lab</li><ul>
         	<li> Charlie Matlack </li>
         	<li> Robert Robinson</li></ul>
         <li> Yuste lab</li><ul>
         <li> Christophe Dupre</li>
         <li> John Szymanski</li></ul>
	  	</ul>
   	  </div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>


	<section>
	<h2>Segmenting regions to extend</h2>
	<div id="left">
		Mumford-Shah image segmention: divide into $k$ regions of constant color, while trying to minimize perimeter of region boundaries
	</div>
	<div id="right"><img src="assets/butterfly_together.png"></div>
		$$\begin{align*}\min_{(R_l),c_l}\frac{1}{2}\sum_{l=1}^k \text{Per}(R_l;\Omega)
			+ \frac{\lambda}{2}\sum_{l=1}^k \int_{R_l} |g(x)-c_l|^2\,dx\end{align*}$$
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Segmenting regions to extend</h2>
	<div id="left">
		Mumford-Shah image segmention: divide into $k$ regions of constant color, while trying to minimize perimeter of region boundaries
	</div>
	<div id="right"><img src="assets/butterfly_together.png"></div>
	<p>
		$$\begin{align*}\min_{u_l}\frac{1}{2}\sum_{l=1}^k \int_\Omega |\nabla u_l|\,dx
			+ \frac{\lambda}{2}\sum_{l=1}^k \int_{\Omega} u_l(x)f_l(x)\,dx\end{align*}
		$$
		with $f_l(x) = |g(x)-c_l|^2$. Assume $c_l$ are known and $\mathbf{u}\in U$:
		$$
		U = \left\{u_l:\sum_l^k u_l(x) = 1, \quad u_l(x) \ge 0, \forall x\in\Omega\right\}
		$$
		Convex in $\mathbf{u}$<br>
		Select color via $v(x) = \text{argmax}_l u_l(x)$ 
	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Mumford-Shah image segmentation</h2>
	<p>
		Chambolle algorithm solves problems:
		$$\min_{x\in X}F(Kx) + G(x)$$
		for convex $F(\cdot):Y\to [0,\infty]$ and $G(\cdot):X\to [0,\infty]$, 
		<br>in primal-dual form:
		$$\min_{x\in X}\max_{y\in Y} \langle Kx, y\rangle - F^*(y) + G(x)$$
		<em>Algorithm:</em>
	</p>
	<ol>
	<li> Initialization: $\tau, \sigma > 0, \theta \in [0,1], (x^0,y^0)\in X\times Y$. Set $\bar{x}^0 = x^0$
	<li> Iterate until convergence: ($n\ge 0$)
		$$\begin{align}
		y^{n+1} &= \pi_{F^*}(y^n + \sigma K \bar{x}^n; \sigma)\\
		x^{n+1} &= \pi_G(x^n - \tau K^* {y}^{n+1}; \tau)\\
		\bar{x}^{n+1} &= x^{n+1} +\theta(x^{n+1} - x^n)
		\end{align}$$
	</ol>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Mumford-Shah image segmentation</h2>
	<p>
		With proximal operator
		$$
		\pi_G(y;\tau) = \text{argmin}_{x}\frac{\|x-y\|_2^2}{2\tau}+G(x)
		$$
		<br>
		Primal-dual MS image segmentation:  
		$$\begin{align}\min_{u=(u_l)_{l=1}^k} \max_{p=(p_l)_{l=1}^k} &\left(\sum_{l=1}^k\langle \nabla u_l, p_l \rangle +\langle u_l, f_l \rangle \right) +\delta_U(u) - \delta_P(p) \end{align}$$
		So,
		<ul>
		<li> $K = \nabla$ (first order forward difference)
		<li> $K^* = -\text{div}$ (first order backward difference)
		<li> $F^*(p) = \delta_P(p)$ with $P=\left\{ p\in Y^k:\|p_l\|_\infty \le \frac{1}{2}\right\}$
		<li> $G(u) = \delta_U(u)$
		</ul>
		Proximal operaters $\pi_{F^*}$ and $\pi_G$ are pixel-wise projection onto convex sets $P$ and $U$
		$\Rightarrow$ Easy to implement on GPU
	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<!-- <section>
	<h2>Mumford-Shah image segmentation</h2>
	<div id="left">
		Proximal operaters $\pi_{F^*}$ and $\pi_G$ are pixel-wise projection onto convex sets $P$ and $U$<br>
		$\Rightarrow$ Easy to implement on GPU
	</div>
	<div id="right"></div>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section> -->

	<section>
	<h2>Segmentation of tracked regions</h2>

		<p>For $K$ reference frames and $L$ iframes, let $f_{ij}(x)$ represent the optic flow error in using reference image $i$ to construct image $j$.  
		$$\begin{align}\min_{u} \max_{p} \sum_{l=1}^L\left(\sum_{k=1}^K\langle \nabla u_{kl}, p_{kl} \rangle +\langle u_{kl}, f_{kl} \rangle \right) +\delta_U(u) - \delta_P(p) \end{align}$$
		</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

	<section>
	<h2>Choosing ref frames</h2>
	<p>
		Want number of ref frames to balance global registration vs registration error <br><br>
		Add a group LASSO penalty for number of reference frames used:
		$$\begin{align}\min_{u=(u_l)_{l=1}^k} \max_{p=(p_l)_{l=1}^k} &\left(\sum_{l=1}^k\langle \nabla u_l, p_l \rangle +\langle u_l, f_l \rangle \right) +\delta_U(u) - \delta_P(p) +\frac{\lambda_2}{2}\sum_{k=1}^K\left(\sum_{l=1}^L \|u_{kl}\|^2_2 \right)^{1/2} \end{align}$$
		The proximal operator $\pi_G$ now becomes:
		$$
		\pi_G(y;\tau) = \text{argmin}_{x}\frac{\|x-y\|_2^2}{2\tau}+\frac{\lambda_2}{2}\sum_{k=1}^K\left(\sum_{l=1}^L \|y_{kl}\|^2_2 \right)^{1/2}+\delta_U(y)
		$$
		Compute $\pi_G$ with ADMM
	</p>
	  <aside class="notes">
	    <span style="color: red"></span> •
	    • <span style="color: green"></span>
	  </aside>
	</section>

    <!--<script src="reveal.js/js/reveal.min.js"></script>-->
    <script src="reveal.js/js/reveal.js"></script>
    <script src="pdfjs/compatibility.js"></script>
    <script src="pdfjs/pdf.js"></script>

    <script src="lib/js/head.min.js"></script>
    <script>
      head.js(
        "lib/js/jquery.min.js",
        "lib/js/jquery.hotkeys.js",
        "lib/js/underscore.min.js",
        "lib/js/swfobject.js",
        "lib/js/dat.gui.js",
        "lib/js/EventEmitter.js",

        "lib/js/three.js",
        "lib/js/three/EffectComposer.js",
        "lib/js/three/RenderPass.js",
        "lib/js/three/BloomPass.js",
        "lib/js/three/ShaderPass.js",
        "lib/js/three/MaskPass.js",

        // three shaders
        "lib/js/three/shaders/CopyShader.js",
        "lib/js/three/shaders/BasicShader.js",
        "lib/js/three/shaders/DotScreenShader.js",
        "lib/js/three/shaders/UnpackDepthRGBAShader.js",
        "lib/js/three/shaders/HorizontalBlurShader.js",
        "lib/js/three/shaders/VerticalBlurShader.js",

        // js files needed for WebGL specific samples (excluding three js)
        "lib/js/J3DI.js",
        "lib/js/J3DIMath.js",
        "lib/js/webgl-utils.js",
        "lib/js/webgl-debug.js",

        // App specific js
        //"js/reveal.min.js",
        "js/stats_bootstrap.js",
        "js/samples.js",
        //"js/dat.gui.bootstrap.js",

		function() {
      Reveal.initialize({
        controls: false,
        progress: true,
        history: true,
        center: false,
        keyboard: true,
        touch: false,
        overview: true,
        mouseWheel: false,
        width: 960,
        height: 720,

        theme: false, // hardcoded with CSS import in <head>
        transition: 'fade', // default/cube/page/concave/zoom/linear/fade/none
        transitionSpeed: 'default', // default/fast/slow

        math: {
          mathjax: 'mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        dependencies: [
          { src: 'reveal.js/lib/js/classList.js',
	    condition: function() { return !document.body.classList; }},
          { src: 'reveal.js/plugin/markdown/marked.js',
	    condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal.js/plugin/markdown/markdown.js',
	    condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal.js/plugin/highlight/highlight.js', async: true,
	    callback: function() { hljs.initHighlightingOnLoad (); }},
          { src: 'reveal.js/plugin/notes/notes.js', async: true,
	    condition: function() { return !!document.body.classList; }},
          { src: 'mymath.js', async: true },
	  { src: 'pdfimgs.js', async: true },
	  { src: 'slideautostart.js', async: true },
        ],
      });
    });
    </script>
  </body>
</html>
